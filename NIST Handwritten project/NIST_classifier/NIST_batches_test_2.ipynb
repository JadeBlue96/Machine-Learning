{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading VGG16 Model ...\n",
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import dataset_novalid\n",
    "import random\n",
    "import os\n",
    "import cache\n",
    "import vgg16\n",
    "from vgg16 import transfer_values_cache\n",
    "from vgg16 import transfer_values_calc\n",
    "vgg16.maybe_download()\n",
    "model=vgg16.VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test images\n",
      "Loading 0 files (Index: 0)\n",
      "Loading 1 files (Index: 1)\n",
      "Loading 2 files (Index: 2)\n",
      "Loading 3 files (Index: 3)\n",
      "Loading 4 files (Index: 4)\n",
      "Loading 5 files (Index: 5)\n",
      "Loading 6 files (Index: 6)\n",
      "Loading 7 files (Index: 7)\n",
      "Loading 8 files (Index: 8)\n",
      "Loading 9 files (Index: 9)\n",
      "Loading A files (Index: 10)\n",
      "Loading A_LOW files (Index: 11)\n",
      "Loading B files (Index: 12)\n",
      "Loading B_LOW files (Index: 13)\n",
      "Loading C files (Index: 14)\n",
      "Loading C_LOW files (Index: 15)\n",
      "Loading D files (Index: 16)\n",
      "Loading D_LOW files (Index: 17)\n",
      "Loading E files (Index: 18)\n",
      "Loading E_LOW files (Index: 19)\n",
      "Loading F files (Index: 20)\n",
      "Loading F_LOW files (Index: 21)\n",
      "Loading G files (Index: 22)\n",
      "Loading G_LOW files (Index: 23)\n",
      "Loading H files (Index: 24)\n",
      "Loading H_LOW files (Index: 25)\n",
      "Loading I files (Index: 26)\n",
      "Loading I_LOW files (Index: 27)\n",
      "Loading J files (Index: 28)\n",
      "Loading J_LOW files (Index: 29)\n",
      "Loading K files (Index: 30)\n",
      "Loading K_LOW files (Index: 31)\n",
      "Loading L files (Index: 32)\n",
      "Loading L_LOW files (Index: 33)\n",
      "Loading M files (Index: 34)\n",
      "Loading M_LOW files (Index: 35)\n",
      "Loading N files (Index: 36)\n",
      "Loading N_LOW files (Index: 37)\n",
      "Loading O files (Index: 38)\n",
      "Loading O_LOW files (Index: 39)\n",
      "Loading P files (Index: 40)\n",
      "Loading P_LOW files (Index: 41)\n",
      "Loading Q files (Index: 42)\n",
      "Loading Q_LOW files (Index: 43)\n",
      "Loading R files (Index: 44)\n",
      "Loading R_LOW files (Index: 45)\n",
      "Loading S files (Index: 46)\n",
      "Loading S_LOW files (Index: 47)\n",
      "Loading T files (Index: 48)\n",
      "Loading T_LOW files (Index: 49)\n",
      "Loading U files (Index: 50)\n",
      "Loading U_LOW files (Index: 51)\n",
      "Loading V files (Index: 52)\n",
      "Loading V_LOW files (Index: 53)\n",
      "Loading W files (Index: 54)\n",
      "Loading W_LOW files (Index: 55)\n",
      "Loading X files (Index: 56)\n",
      "Loading X_LOW files (Index: 57)\n",
      "Loading Y files (Index: 58)\n",
      "Loading Y_LOW files (Index: 59)\n",
      "Loading Z files (Index: 60)\n",
      "Loading Z_LOW files (Index: 61)\n"
     ]
    }
   ],
   "source": [
    "classes=['0','1','2','3','4','5','6','7','8','9','A','A_LOW','B','B_LOW','C','C_LOW','D','D_LOW','E','E_LOW','F','F_LOW','G',\n",
    "         'G_LOW','H','H_LOW','I','I_LOW','J','J_LOW','K','K_LOW','L','L_LOW','M','M_LOW','N','N_LOW','O','O_LOW','P','P_LOW','Q',\n",
    "         'Q_LOW','R','R_LOW','S','S_LOW','T','T_LOW','U','U_LOW','V','V_LOW','W','W_LOW','X','X_LOW','Y','Y_LOW','Z','Z_LOW']\n",
    "class_numbers=list(range(62))\n",
    "num_classes=len(classes)\n",
    "num_channels=3\n",
    "img_size = 128\n",
    "#custom paths\n",
    "test_path = 'C:/Users/Konstantin/Machine Learning/NIST Handwritten project/batches/batch_2/test'\n",
    "test_data = dataset_novalid.read_test_set(test_path, img_size,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Test-set:\t\t29172\n"
     ]
    }
   ],
   "source": [
    "images_test,labels_test,cls_test,ids_test=test_data.test.images,test_data.test.labels,test_data.test.cls,test_data.test.ids\n",
    "print(\"Size of:\")\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(test_data.test.labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cache_path= 'C:/Users/Konstantin/Machine Learning/NIST Handwritten project/cache_NIST'\n",
    "file_path_cache_train = os.path.join(cache_path, 'vgg16_train_full_pool5_2.pkl')\n",
    "file_path_cache_test = os.path.join('D:/Konstantin/cache/batch_2.pkl')\n",
    "images_train=os.path.join(cache_path, 'images_train_2.pkl')\n",
    "labels_train=os.path.join(cache_path, 'labels_train_2.pkl')\n",
    "cls_train=os.path.join(cache_path, 'cls_train_2.pkl')\n",
    "ids_train=os.path.join(cache_path, 'ids_train_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing VGG 16 transfer-values for training-images ...\n",
      "- Data loaded from cache-file: C:/Users/Konstantin/Machine Learning/NIST Handwritten project/cache_NIST\\vgg16_train_full_pool5_2.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing VGG 16 transfer-values for training-images ...\")\n",
    "\n",
    "transfer_values_train = transfer_values_cache(cache_path=file_path_cache_train,\n",
    "                                              images=images_train,\n",
    "                                              model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing VGG 16 transfer-values for test-images ...\n",
      "- Processing image:  29172 / 29172\n",
      "- Data saved to cache-file: D:/Konstantin/cache/batch_2.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing VGG 16 transfer-values for test-images ...\")\n",
    "\n",
    "transfer_values_test = transfer_values_cache(cache_path=file_path_cache_test,\n",
    "                                             images=images_test,\n",
    "                                             model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transfer_values_train_r=transfer_values_train.reshape(100000,8192)\n",
    "transfer_values_test_r=transfer_values_test.reshape(len(test_data.test.labels),8192)\n",
    "transfer_len=8192\n",
    "cls_test = np.array([label.argmax() for label in labels_test])\n",
    "fc_size=2048\n",
    "keep_prob=0.5\n",
    "tf_reg=2e-3\n",
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))\n",
    "def new_fc_layer(input,          \n",
    "                 num_inputs,     \n",
    "                 num_outputs,\n",
    "                 keep_prob,\n",
    "                 l2_loss,\n",
    "                 use_relu=True,\n",
    "                use_dropout=True,\n",
    "                ): \n",
    "\n",
    "    \n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "    result_loss=l2_loss*tf.nn.l2_loss(weights)\n",
    "   \n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    \n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    if use_dropout:\n",
    "        layer = tf.nn.dropout(layer,keep_prob)\n",
    "    return layer,result_loss\n",
    "def random_batch():\n",
    "    \n",
    "    num_images = len(transfer_values_train_r)\n",
    "\n",
    "    \n",
    "    idx = np.random.choice(num_images,\n",
    "                           size=train_batch_size,\n",
    "                           replace=False)\n",
    "\n",
    "    \n",
    "    x_batch = transfer_values_train_r[idx]\n",
    "    y_batch = labels_train[idx]\n",
    "\n",
    "    return x_batch, y_batch\n",
    "x = tf.placeholder(tf.float32, shape=[None, transfer_len], name='x')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)\n",
    "\n",
    "layer_fc1,fc1_loss = new_fc_layer(input=x,\n",
    "                         num_inputs=transfer_len,\n",
    "                         num_outputs=fc_size,\n",
    "                         l2_loss=tf_reg,\n",
    "                         keep_prob=keep_prob,\n",
    "                         use_relu=True,\n",
    "                        use_dropout=False)\n",
    "layer_fc2,fc2_loss = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=fc_size,\n",
    "                         l2_loss=tf_reg,\n",
    "                         keep_prob=keep_prob,\n",
    "                         use_relu=True,\n",
    "                        use_dropout=False)\n",
    "\n",
    "layer_fc3,fc3_loss = new_fc_layer(input=layer_fc2,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=num_classes,\n",
    "                         l2_loss=tf_reg,\n",
    "                         keep_prob=keep_prob,\n",
    "                         use_relu=False,use_dropout=False)\n",
    "y_pred = tf.nn.softmax(layer_fc3)\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc3,\n",
    "                                                        labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)+fc1_loss+fc2_loss+fc3_loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "feed_dict_test = {x: transfer_values_test_r,\n",
    "                  y_true: labels_test}\n",
    "feed_dict_train = {x: transfer_values_train_r,\n",
    "                  y_true: labels_train}\n",
    "def print_accuracy_test():\n",
    "    acc = session.run(accuracy, feed_dict=feed_dict_test)\n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(acc))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cls_pred):\n",
    "    \n",
    "    cm = confusion_matrix(y_true=cls_test,  \n",
    "                          y_pred=cls_pred)  \n",
    "\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "       \n",
    "        class_name = \"({}) {}\".format(i, classes[i])\n",
    "        print(cm[i, :], class_name)\n",
    "\n",
    "    \n",
    "    class_numbers = [\" ({0})\".format(i) for i in range(num_classes)]\n",
    "    print(\"\".join(class_numbers))\n",
    "def plot_example_errors(cls_pred, correct):\n",
    "    \n",
    "    incorrect = (correct == False)\n",
    "    \n",
    "   \n",
    "    images = images_test[incorrect]\n",
    "    \n",
    "    \n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    \n",
    "    cls_true = cls_test[incorrect]\n",
    "\n",
    "    n = min(9, len(images))\n",
    "    \n",
    "    # Plot the first n images.\n",
    "    plot_images(images=images[0:n],\n",
    "                cls_true=cls_true[0:n],\n",
    "                cls_pred=cls_pred[0:n])\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "def predict_cls(transfer_values, labels, cls_true):\n",
    "    \n",
    "    num_images = len(transfer_values)\n",
    "\n",
    "    \n",
    "    cls_pred = np.zeros(shape=num_images, dtype=np.int)\n",
    "\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    while i < num_images:\n",
    "        \n",
    "        j = min(i + batch_size, num_images)   \n",
    "        feed_dict = {x: transfer_values[i:j],\n",
    "                     y_true: labels[i:j]}       \n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)      \n",
    "        i = j\n",
    "        \n",
    "    \n",
    "    correct = (cls_true == cls_pred)\n",
    "    print (correct)\n",
    "    return correct, cls_pred\n",
    "def predict_cls_test():\n",
    "    return predict_cls(transfer_values = transfer_values_test_r,\n",
    "                       labels = labels_test,\n",
    "                       cls_true = cls_test)\n",
    "def classification_accuracy(correct):\n",
    "    return correct.mean(), correct.sum()\n",
    "def print_test_accuracy(show_example_errors=False,\n",
    "                        show_confusion_matrix=False):\n",
    "\n",
    "    correct, cls_pred = predict_cls_test()\n",
    "    acc, num_correct = classification_accuracy(correct)\n",
    "    num_images = len(correct)\n",
    "\n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, num_correct, num_images))\n",
    "    if show_example_errors:\n",
    "        print(\"Example errors:\")\n",
    "        plot_example_errors(cls_pred=cls_pred, correct=correct)\n",
    "    if show_confusion_matrix:\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plot_confusion_matrix(cls_pred=cls_pred)\n",
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        print(\"no images to show\")\n",
    "        return \n",
    "    else:\n",
    "        random_indices = random.sample(range(len(images)), min(len(images), 9))\n",
    "        \n",
    "        \n",
    "    images, cls_true  = zip(*[(images[i], cls_true[i]) for i in random_indices])\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "\n",
    "        ax.imshow(images[i].reshape(img_size, img_size, num_channels))\n",
    "\n",
    "\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "save_path='checkpoints/best_test-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/best_test-2\n",
      "Model restored.\n",
      "Accuracy on test-set: 84.2%\n",
      "[ True  True False ...,  True  True False]\n",
      "Accuracy on Test-Set: 84.2% (24551 / 29172)\n",
      "Confusion Matrix:\n",
      "[3570    0    1    1    0    0   10    0    8    2    2    6    7   10   11\n",
      "   12  111    3    0   10    0    1    9   10    0    0    0    0    1    1\n",
      "    0    0    0    0    0    0    0    0   98 1101    2    3   17    0    0\n",
      "    0    0    1    0    0    6   12    3    4    0    0    0    0    0    1\n",
      "    0    0] (0) 0\n",
      "[   0 4635    0    1    4    2    2    3    1    1    1    0    0    0    1\n",
      "    0    0    0    0    0    0    2    0    0    0    4   34  638    6   10\n",
      "    0    0   18  103    0    0    0    0    0    0    0    2    0    3    0\n",
      "    3    0    0    0    2    0    1    1    2    0    0    0    3    0   23\n",
      "    1    1] (1) 1\n",
      "[   3    0 4080   13    2    4    9    9    9    4    3   41    1    1    4\n",
      "    7    7    8    6   12    7   14    6   36    0    0    0    2    1    8\n",
      "    1    0   10    0    0    0    0    1    0    1    0    2   16   13   10\n",
      "    1    5   13    0    3    5    2    1    4    1    3    3   10    2   10\n",
      "  157  250] (2) 2\n",
      "[   1    0   52 4580    3   42    1    2   21   17    0    5   38    9    0\n",
      "    4    8    0   20    3    0   12    7   69    0    0    1    0   13    8\n",
      "    0    0    0    0    0    0    0    0    0    3    1    0    2    5    1\n",
      "    0   31   76    0    2    0    0    0    0    0    2    0    1    5    2\n",
      "    4   41] (3) 3\n",
      "[   0    1    7    1 4092    5    0    3    3    8   24    4    1    1    0\n",
      "    0    0    6    2    0    3   28    0    3   56    1    0    0    5    2\n",
      "   11   18    1    0    0    2    5    0    0    0    0    1    3   21    1\n",
      "    0    1    1    2   23    8   38    8    6    5   12    7   17   59   99\n",
      "    3    6] (4) 4\n",
      "[   2    1    8   67    1 3594    4    0   13    0    0    1    4    5    0\n",
      "    1    1    0   24    0    5    9    5   12    0    1    0    0   47    9\n",
      "    1    0    0    1    1    0    0    0    1    1    0    0    0    1    0\n",
      "    0   41  220    0    0    0    1    1    1    0    1    0    1    2    1\n",
      "    1   13] (5) 5\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (6) 6\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (7) 7\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (8) 8\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (9) 9\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (10) A\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (11) A_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (12) B\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (13) B_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (14) C\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (15) C_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (16) D\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (17) D_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (18) E\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (19) E_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (20) F\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (21) F_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (22) G\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (23) G_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (24) H\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (25) H_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (26) I\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (27) I_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (28) J\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (29) J_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (30) K\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (31) K_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (32) L\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (33) L_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (34) M\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (35) M_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (36) N\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (37) N_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (38) O\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (39) O_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (40) P\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (41) P_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (42) Q\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (43) Q_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (44) R\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (45) R_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (46) S\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (47) S_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (48) T\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (49) T_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (50) U\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (51) U_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (52) V\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (53) V_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (54) W\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (55) W_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (56) X\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (57) X_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (58) Y\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (59) Y_LOW\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (60) Z\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (61) Z_LOW\n",
      " (0) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45) (46) (47) (48) (49) (50) (51) (52) (53) (54) (55) (56) (57) (58) (59) (60) (61)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    saver=tf.train.Saver()\n",
    "    saver.restore(session, save_path)\n",
    "    print(\"Model restored.\")\n",
    "    print_accuracy_test()\n",
    "    print_test_accuracy(show_example_errors=False,\n",
    "                        show_confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
